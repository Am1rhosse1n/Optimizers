# Optimizers
Implementations of some important Optimizers Such as Gradient Descent, Adam, Adadelta, RMS Prob and Nestrov Accelerated Gradient (NAG) used in 3 Layer Perceptron from scratch without using any libraries.
